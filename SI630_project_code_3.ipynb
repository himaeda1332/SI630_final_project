{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SI630_project_code_3.ipynb","provenance":[],"collapsed_sections":["Hgs5CoXawuOR","zUMKIFesxFiF","qFESFzCzlGxJ","P4fegm_blJcK"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1UTz4lMOkSYSBwu475qolVhbibbZ0vqFi","authorship_tag":"ABX9TyMqhMaG7tfTtRVWMPypOI1v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"abf08b10c04b4e209ab8d9f668193a3c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2d824d016e3d41a6a3b5e53c4de1f266","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dca5e6e2de4a4ea9904025fcfb0db2b2","IPY_MODEL_58e5fbe885bf4b628b82b0631b0ccb6d"]}},"2d824d016e3d41a6a3b5e53c4de1f266":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dca5e6e2de4a4ea9904025fcfb0db2b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e0a6f6ed062647868da3780f3bc4b38f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1199,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1199,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1f03cdd90d2a4f9faf535439ded9e53c"}},"58e5fbe885bf4b628b82b0631b0ccb6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_01f8bd776c6547a2a6a10f93e526dca1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20k/1.20k [00:00&lt;00:00, 4.91kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6115df69926d433aa85f7fc2e0573955"}},"e0a6f6ed062647868da3780f3bc4b38f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1f03cdd90d2a4f9faf535439ded9e53c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01f8bd776c6547a2a6a10f93e526dca1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6115df69926d433aa85f7fc2e0573955":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b469198d0f70420fb6752d97f797fcc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7aff0d492d2f4c809e6cec1ba8f16244","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e5296bb8d05f4dcab721ee219036e3b2","IPY_MODEL_efa3d509acbe44a9aa94264634c9bf33"]}},"7aff0d492d2f4c809e6cec1ba8f16244":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5296bb8d05f4dcab721ee219036e3b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e4dd87c77fe244c5a5815d024a5d0221","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":891691430,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":891691430,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6fd5b27b204545cfa413b09a0c4cb6f1"}},"efa3d509acbe44a9aa94264634c9bf33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4d63896be32b470b841380de29f6f10c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 892M/892M [00:21&lt;00:00, 41.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1ccca099405545999d68225167c82d73"}},"e4dd87c77fe244c5a5815d024a5d0221":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6fd5b27b204545cfa413b09a0c4cb6f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d63896be32b470b841380de29f6f10c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1ccca099405545999d68225167c82d73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bc282d29f1e48a297c5257d034aeed2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_75a164f69bae4dc684c67eeb1d3f8a1c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_85916c99d2b54286965fa3ab649b32c9","IPY_MODEL_696b697e74394393877c880c4fc3421c"]}},"75a164f69bae4dc684c67eeb1d3f8a1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85916c99d2b54286965fa3ab649b32c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c5c092f24dba42feb18726fc74925042","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ed082d3e76a4433b90cd7d565049eee7"}},"696b697e74394393877c880c4fc3421c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ad4a63b6f74a48da91aeb2cf8592bb52","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 792k/792k [00:01&lt;00:00, 666kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2b91faa2b6224e269e1ed0972e289d88"}},"c5c092f24dba42feb18726fc74925042":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ed082d3e76a4433b90cd7d565049eee7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad4a63b6f74a48da91aeb2cf8592bb52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2b91faa2b6224e269e1ed0972e289d88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d0a6d75488ce4cfe8a87bbdd046476ba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0ec160c91c7842a9b14496546862442d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_aed10ad5d75646428f62f985077e77f6","IPY_MODEL_f28871722ad641ff93f9a889187eaa90"]}},"0ec160c91c7842a9b14496546862442d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aed10ad5d75646428f62f985077e77f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e0b53d2d3e6a4603880ac34f68c634c7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1389353,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1389353,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c54e9438675e4c72a04a046b3f19f0ad"}},"f28871722ad641ff93f9a889187eaa90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_816becee668b4016b2775d241920a0ea","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.39M/1.39M [00:00&lt;00:00, 3.27MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9720bb23c7ab4a2cb35f0809dcc1dc26"}},"e0b53d2d3e6a4603880ac34f68c634c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c54e9438675e4c72a04a046b3f19f0ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"816becee668b4016b2775d241920a0ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9720bb23c7ab4a2cb35f0809dcc1dc26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb117f58cb644cd9bbc341d0e66fdf73":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d6a57e8b376645acaf24a5cc36fed9cc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6b18e4a6650d4b899f2444383ab65880","IPY_MODEL_843f2f0f5f2c4831a4ef9143da4a15c1"]}},"d6a57e8b376645acaf24a5cc36fed9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b18e4a6650d4b899f2444383ab65880":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ff2eb416698a437ba1143c7d6ee33b8a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d265f54aacf64c8fa0e047ee093d2008"}},"843f2f0f5f2c4831a4ef9143da4a15c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_031550d4b4e74ef4a9a4ea997c38d945","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:01&lt;00:00, 211kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_12ffc67a84e549f8a03941846d326904"}},"ff2eb416698a437ba1143c7d6ee33b8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d265f54aacf64c8fa0e047ee093d2008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"031550d4b4e74ef4a9a4ea997c38d945":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"12ffc67a84e549f8a03941846d326904":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fa96e58ab20e4adc8398e09b8f3a5c72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0275d9e9d1cc455182044afdd1c299ff","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f4caf189aa9440a985a87500f3056171","IPY_MODEL_dc4f3fee75c945c2a39236660f4288d7"]}},"0275d9e9d1cc455182044afdd1c299ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f4caf189aa9440a985a87500f3056171":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_49c36155bcf64a33a5eb7fd70b6bb263","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5f958e68c7e403c9ae37b993d5073b7"}},"dc4f3fee75c945c2a39236660f4288d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f5068605397641c7978c56caa8ef3fde","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 982B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a5348f5afc7b40ada90816c763c6b764"}},"49c36155bcf64a33a5eb7fd70b6bb263":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b5f958e68c7e403c9ae37b993d5073b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5068605397641c7978c56caa8ef3fde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a5348f5afc7b40ada90816c763c6b764":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56bce91d50524c8c8b5afdc5ae48075d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c99273997f69402e9db76589910bd14d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2758fe03dea54cf8b1c3123ae30c9bbd","IPY_MODEL_a81f889ad21344b29c50877dac7e66ac"]}},"c99273997f69402e9db76589910bd14d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2758fe03dea54cf8b1c3123ae30c9bbd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f8053078a6694df88dad36d2d2136e76","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a84f065d34ce4e9fa8f8fb6d866440cf"}},"a81f889ad21344b29c50877dac7e66ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9514313fd15d4362888d9ad986f9854c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.39MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15268cb1754a4d7c841c57bebef73b93"}},"f8053078a6694df88dad36d2d2136e76":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a84f065d34ce4e9fa8f8fb6d866440cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9514313fd15d4362888d9ad986f9854c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"15268cb1754a4d7c841c57bebef73b93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8a0e754b395046048b8a08fa017e25ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_72d7ae2aff1e4953b0ed50d6f2bcabb5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5517a94f4e1a46a587ed036b1e4beb72","IPY_MODEL_a64fabe17dd34f898ae7872b11b2b36c"]}},"72d7ae2aff1e4953b0ed50d6f2bcabb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5517a94f4e1a46a587ed036b1e4beb72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bb66c1859fad4f5e9a2b6c7df33b62c0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ee47569e14f5404b99cde357182e34d1"}},"a64fabe17dd34f898ae7872b11b2b36c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c85765bfef0a42bfb3736e7f03e195bc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:00&lt;00:00, 16.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f308cecdfe95410981c723ac8de5d3d3"}},"bb66c1859fad4f5e9a2b6c7df33b62c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ee47569e14f5404b99cde357182e34d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c85765bfef0a42bfb3736e7f03e195bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f308cecdfe95410981c723ac8de5d3d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a7fb51f904545b485ec07efb00aa373":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3f137185bb9646d7af24ea830b32aff6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c3145a6956db46b4972030d662b6eadf","IPY_MODEL_e43396fcd6ce4aa68ddccb71570fa6f8"]}},"3f137185bb9646d7af24ea830b32aff6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c3145a6956db46b4972030d662b6eadf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9100dac0feac4569a64463501351567b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_80bcda2cae024cc0bfe9ab28ac608eda"}},"e43396fcd6ce4aa68ddccb71570fa6f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b9af5e924d046f481120de07f2103a2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:04&lt;00:00, 61.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f409d49bc78c4e14aae693f36321efff"}},"9100dac0feac4569a64463501351567b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"80bcda2cae024cc0bfe9ab28ac608eda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b9af5e924d046f481120de07f2103a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f409d49bc78c4e14aae693f36321efff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvZK9OhcrmEK","executionInfo":{"status":"ok","timestamp":1619593528548,"user_tz":240,"elapsed":8809,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"b2f23b7f-3824-4210-e3e4-f5d787781b1b"},"source":["!pip install transformers -q\n","!pip install sentencepiece"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 2.1MB 7.7MB/s \n","\u001b[K     |████████████████████████████████| 3.3MB 37.7MB/s \n","\u001b[K     |████████████████████████████████| 901kB 48.8MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 7.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B4Wy4EBeTMm4","executionInfo":{"status":"ok","timestamp":1619593531076,"user_tz":240,"elapsed":293,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"e3ac4b0d-ca85-4e97-84a0-c51a8fd1e458"},"source":["!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Wed Apr 28 07:05:30 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8TEL0TdaVpf7"},"source":["# 1.Import libraries"]},{"cell_type":"code","metadata":{"id":"xPqAEDmdr9SP","executionInfo":{"status":"ok","timestamp":1619593546909,"user_tz":240,"elapsed":4662,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["# Importing stock libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn\n","from torch import optim\n","\n","from tqdm.notebook import tqdm\n","\n","from torch.optim import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","from collections import defaultdict\n","import gc\n","\n","# Importing the T5 modules from huggingface/transformers\n","import sentencepiece\n","from transformers import AutoModelWithLMHead, AutoTokenizer\n","from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6-z_cIl72l0","executionInfo":{"status":"ok","timestamp":1619593547490,"user_tz":240,"elapsed":2653,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["RANDOM_SEED = 42\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267,"referenced_widgets":["abf08b10c04b4e209ab8d9f668193a3c","2d824d016e3d41a6a3b5e53c4de1f266","dca5e6e2de4a4ea9904025fcfb0db2b2","58e5fbe885bf4b628b82b0631b0ccb6d","e0a6f6ed062647868da3780f3bc4b38f","1f03cdd90d2a4f9faf535439ded9e53c","01f8bd776c6547a2a6a10f93e526dca1","6115df69926d433aa85f7fc2e0573955","b469198d0f70420fb6752d97f797fcc0","7aff0d492d2f4c809e6cec1ba8f16244","e5296bb8d05f4dcab721ee219036e3b2","efa3d509acbe44a9aa94264634c9bf33","e4dd87c77fe244c5a5815d024a5d0221","6fd5b27b204545cfa413b09a0c4cb6f1","4d63896be32b470b841380de29f6f10c","1ccca099405545999d68225167c82d73","8bc282d29f1e48a297c5257d034aeed2","75a164f69bae4dc684c67eeb1d3f8a1c","85916c99d2b54286965fa3ab649b32c9","696b697e74394393877c880c4fc3421c","c5c092f24dba42feb18726fc74925042","ed082d3e76a4433b90cd7d565049eee7","ad4a63b6f74a48da91aeb2cf8592bb52","2b91faa2b6224e269e1ed0972e289d88","d0a6d75488ce4cfe8a87bbdd046476ba","0ec160c91c7842a9b14496546862442d","aed10ad5d75646428f62f985077e77f6","f28871722ad641ff93f9a889187eaa90","e0b53d2d3e6a4603880ac34f68c634c7","c54e9438675e4c72a04a046b3f19f0ad","816becee668b4016b2775d241920a0ea","9720bb23c7ab4a2cb35f0809dcc1dc26"]},"id":"Y50k1Lteeq8d","executionInfo":{"status":"ok","timestamp":1619593579552,"user_tz":240,"elapsed":32236,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"3b449101-537a-4883-89ca-8d45d9eb8b3e"},"source":["model = AutoModelWithLMHead.from_pretrained(\"t5-base\")\n","tokenizer_summary = AutoTokenizer.from_pretrained(\"t5-base\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:762: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abf08b10c04b4e209ab8d9f668193a3c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1199.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b469198d0f70420fb6752d97f797fcc0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=891691430.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8bc282d29f1e48a297c5257d034aeed2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0a6d75488ce4cfe8a87bbdd046476ba","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["cb117f58cb644cd9bbc341d0e66fdf73","d6a57e8b376645acaf24a5cc36fed9cc","6b18e4a6650d4b899f2444383ab65880","843f2f0f5f2c4831a4ef9143da4a15c1","ff2eb416698a437ba1143c7d6ee33b8a","d265f54aacf64c8fa0e047ee093d2008","031550d4b4e74ef4a9a4ea997c38d945","12ffc67a84e549f8a03941846d326904","fa96e58ab20e4adc8398e09b8f3a5c72","0275d9e9d1cc455182044afdd1c299ff","f4caf189aa9440a985a87500f3056171","dc4f3fee75c945c2a39236660f4288d7","49c36155bcf64a33a5eb7fd70b6bb263","b5f958e68c7e403c9ae37b993d5073b7","f5068605397641c7978c56caa8ef3fde","a5348f5afc7b40ada90816c763c6b764","56bce91d50524c8c8b5afdc5ae48075d","c99273997f69402e9db76589910bd14d","2758fe03dea54cf8b1c3123ae30c9bbd","a81f889ad21344b29c50877dac7e66ac","f8053078a6694df88dad36d2d2136e76","a84f065d34ce4e9fa8f8fb6d866440cf","9514313fd15d4362888d9ad986f9854c","15268cb1754a4d7c841c57bebef73b93"]},"id":"bKpckGKDsAZ3","executionInfo":{"status":"ok","timestamp":1619593581646,"user_tz":240,"elapsed":31830,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"5b64f402-5e7b-4569-af1b-9e29b57a8892"},"source":["tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb117f58cb644cd9bbc341d0e66fdf73","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa96e58ab20e4adc8398e09b8f3a5c72","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56bce91d50524c8c8b5afdc5ae48075d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DsWt58VLWub5"},"source":["# 2.Load dataset"]},{"cell_type":"code","metadata":{"id":"rkmi_sJQW1PB","executionInfo":{"status":"ok","timestamp":1619593587201,"user_tz":240,"elapsed":167,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["INPUT_PATH = 'input/'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"QXpCUivCWx2D","executionInfo":{"status":"ok","timestamp":1619593616821,"user_tz":240,"elapsed":1251,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"b12b753f-775f-4609-a946-71b582499fa8"},"source":["df_info_all_v2 = pd.read_csv(INPUT_PATH + 'df_all_final.csv')\n","df_info_all_v2.head()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>uuid</th>\n","      <th>author</th>\n","      <th>country</th>\n","      <th>published</th>\n","      <th>image</th>\n","      <th>site</th>\n","      <th>site_category</th>\n","      <th>page_view</th>\n","      <th>fb_comment</th>\n","      <th>fb_likes</th>\n","      <th>fb_shares</th>\n","      <th>linkedin</th>\n","      <th>pinterest</th>\n","      <th>url</th>\n","      <th>date</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>weekday</th>\n","      <th>hour</th>\n","      <th>minute</th>\n","      <th>seccont</th>\n","      <th>noweek</th>\n","      <th>doc_topic</th>\n","      <th>log_share</th>\n","      <th>num_title_words</th>\n","      <th>num_words</th>\n","      <th>num_words_clean</th>\n","      <th>country_model</th>\n","      <th>site_model</th>\n","      <th>country_label</th>\n","      <th>site_label</th>\n","      <th>log_page</th>\n","      <th>positive</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9b265eb1f08a07a093f4e415f4878b5ccec71b0b</td>\n","      <td>David E. Sanger, Julian E. Barnes and Nicole P...</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:58:58.000+02:00</td>\n","      <td>0</td>\n","      <td>www.msn.com</td>\n","      <td>tech</td>\n","      <td>578.0</td>\n","      <td>17</td>\n","      <td>28</td>\n","      <td>107</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>https://www.msn.com/en-us/news/politics/white-...</td>\n","      <td>2021-03-15 01:58:58</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>58</td>\n","      <td>6314338.0</td>\n","      <td>11</td>\n","      <td>14</td>\n","      <td>4.682131</td>\n","      <td>11</td>\n","      <td>54</td>\n","      <td>26</td>\n","      <td>US</td>\n","      <td>www.msn.com</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>6.361302</td>\n","      <td>0.012595</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0d49c01d17674d04754b87cb7c5167a589934e61</td>\n","      <td>Marina Pitofsky</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:58:57.000+02:00</td>\n","      <td>0</td>\n","      <td>www.msn.com</td>\n","      <td>tech</td>\n","      <td>578.0</td>\n","      <td>396</td>\n","      <td>3438</td>\n","      <td>84</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>https://www.msn.com/en-us/news/politics/rachel...</td>\n","      <td>2021-03-15 01:58:57</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>58</td>\n","      <td>6314337.0</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>4.442651</td>\n","      <td>10</td>\n","      <td>310</td>\n","      <td>177</td>\n","      <td>US</td>\n","      <td>www.msn.com</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>6.361302</td>\n","      <td>0.174687</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ad1d5c62864864e9ae3d36b487cc322f5bcbaf9c</td>\n","      <td>Kevin Rector</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:51:00.000+02:00</td>\n","      <td>1</td>\n","      <td>www.latimes.com</td>\n","      <td>tech</td>\n","      <td>14.7</td>\n","      <td>19</td>\n","      <td>30</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>https://www.latimes.com/california/story/2021-...</td>\n","      <td>2021-03-15 01:51:00</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>51</td>\n","      <td>6313860.0</td>\n","      <td>11</td>\n","      <td>5</td>\n","      <td>3.135494</td>\n","      <td>12</td>\n","      <td>196</td>\n","      <td>104</td>\n","      <td>US</td>\n","      <td>other</td>\n","      <td>9</td>\n","      <td>3</td>\n","      <td>2.753661</td>\n","      <td>0.004682</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9314d252503ee1f811c29627ab629c5b799ef103</td>\n","      <td>Kim Lyons</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:29:00.000+02:00</td>\n","      <td>1</td>\n","      <td>www.theverge.com</td>\n","      <td>tech</td>\n","      <td>31.9</td>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>25</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>https://www.theverge.com/2021/3/14/22326012/ne...</td>\n","      <td>2021-03-15 01:29:00</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>6312540.0</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>3.258097</td>\n","      <td>14</td>\n","      <td>415</td>\n","      <td>182</td>\n","      <td>US</td>\n","      <td>other</td>\n","      <td>9</td>\n","      <td>3</td>\n","      <td>3.493473</td>\n","      <td>0.006911</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>cda04303d248fcdab575934d10d33b957e9691d6</td>\n","      <td>Des Erasmus</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:22:32.000+02:00</td>\n","      <td>0</td>\n","      <td>www.msn.com</td>\n","      <td>tech</td>\n","      <td>578.0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>https://www.msn.com/en-za/news/other/the-lord-...</td>\n","      <td>2021-03-15 01:22:32</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>6312152.0</td>\n","      <td>11</td>\n","      <td>6</td>\n","      <td>2.484907</td>\n","      <td>16</td>\n","      <td>2262</td>\n","      <td>937</td>\n","      <td>US</td>\n","      <td>www.msn.com</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>6.361302</td>\n","      <td>0.005006</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       uuid  ...  positive\n","0  9b265eb1f08a07a093f4e415f4878b5ccec71b0b  ...  0.012595\n","1  0d49c01d17674d04754b87cb7c5167a589934e61  ...  0.174687\n","2  ad1d5c62864864e9ae3d36b487cc322f5bcbaf9c  ...  0.004682\n","3  9314d252503ee1f811c29627ab629c5b799ef103  ...  0.006911\n","4  cda04303d248fcdab575934d10d33b957e9691d6  ...  0.005006\n","\n","[5 rows x 34 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ijUneJrXSsc","executionInfo":{"status":"ok","timestamp":1619593622569,"user_tz":240,"elapsed":3818,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"c3573076-7b08-4db8-a098-9c74004fb0ee"},"source":["df_text_all = pd.read_csv(INPUT_PATH + 'text_all_clean.csv')\n","df_text_all.columns"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['uuid', 'title', 'text', 'clean_title', 'clean_text', 'num_words',\n","       'num_words_clean'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"OtcKkk9cXmky","executionInfo":{"status":"ok","timestamp":1619593622569,"user_tz":240,"elapsed":953,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"6adf802f-ced5-477e-9cfe-c92291288bdd"},"source":["df_info_all_v2 = df_info_all_v2.merge(df_text_all[['uuid', 'clean_title', 'clean_text']], left_on='uuid', right_on='uuid')\n","df_info_all_v2.head(3)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>uuid</th>\n","      <th>author</th>\n","      <th>country</th>\n","      <th>published</th>\n","      <th>image</th>\n","      <th>site</th>\n","      <th>site_category</th>\n","      <th>page_view</th>\n","      <th>fb_comment</th>\n","      <th>fb_likes</th>\n","      <th>fb_shares</th>\n","      <th>linkedin</th>\n","      <th>pinterest</th>\n","      <th>url</th>\n","      <th>date</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>weekday</th>\n","      <th>hour</th>\n","      <th>minute</th>\n","      <th>seccont</th>\n","      <th>noweek</th>\n","      <th>doc_topic</th>\n","      <th>log_share</th>\n","      <th>num_title_words</th>\n","      <th>num_words</th>\n","      <th>num_words_clean</th>\n","      <th>country_model</th>\n","      <th>site_model</th>\n","      <th>country_label</th>\n","      <th>site_label</th>\n","      <th>log_page</th>\n","      <th>positive</th>\n","      <th>clean_title</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9b265eb1f08a07a093f4e415f4878b5ccec71b0b</td>\n","      <td>David E. Sanger, Julian E. Barnes and Nicole P...</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:58:58.000+02:00</td>\n","      <td>0</td>\n","      <td>www.msn.com</td>\n","      <td>tech</td>\n","      <td>578.0</td>\n","      <td>17</td>\n","      <td>28</td>\n","      <td>107</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>https://www.msn.com/en-us/news/politics/white-...</td>\n","      <td>2021-03-15 01:58:58</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>58</td>\n","      <td>6314338.0</td>\n","      <td>11</td>\n","      <td>14</td>\n","      <td>4.682131</td>\n","      <td>11</td>\n","      <td>54</td>\n","      <td>26</td>\n","      <td>US</td>\n","      <td>www.msn.com</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>6.361302</td>\n","      <td>0.012595</td>\n","      <td>white house weigh new cybersecurity approach f...</td>\n","      <td>washington sophisticated hack pull russia chin...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0d49c01d17674d04754b87cb7c5167a589934e61</td>\n","      <td>Marina Pitofsky</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:58:57.000+02:00</td>\n","      <td>0</td>\n","      <td>www.msn.com</td>\n","      <td>tech</td>\n","      <td>578.0</td>\n","      <td>396</td>\n","      <td>3438</td>\n","      <td>84</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>https://www.msn.com/en-us/news/politics/rachel...</td>\n","      <td>2021-03-15 01:58:57</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>58</td>\n","      <td>6314337.0</td>\n","      <td>11</td>\n","      <td>1</td>\n","      <td>4.442651</td>\n","      <td>10</td>\n","      <td>310</td>\n","      <td>177</td>\n","      <td>US</td>\n","      <td>www.msn.com</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>6.361302</td>\n","      <td>0.174687</td>\n","      <td>rachel maddow win grammy audio recording book</td>\n","      <td>© getty image rachel maddow win grammy audio r...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ad1d5c62864864e9ae3d36b487cc322f5bcbaf9c</td>\n","      <td>Kevin Rector</td>\n","      <td>US</td>\n","      <td>2021-03-15T01:51:00.000+02:00</td>\n","      <td>1</td>\n","      <td>www.latimes.com</td>\n","      <td>tech</td>\n","      <td>14.7</td>\n","      <td>19</td>\n","      <td>30</td>\n","      <td>22</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>https://www.latimes.com/california/story/2021-...</td>\n","      <td>2021-03-15 01:51:00</td>\n","      <td>2021</td>\n","      <td>3</td>\n","      <td>15</td>\n","      <td>Mon</td>\n","      <td>1</td>\n","      <td>51</td>\n","      <td>6313860.0</td>\n","      <td>11</td>\n","      <td>5</td>\n","      <td>3.135494</td>\n","      <td>12</td>\n","      <td>196</td>\n","      <td>104</td>\n","      <td>US</td>\n","      <td>other</td>\n","      <td>9</td>\n","      <td>3</td>\n","      <td>2.753661</td>\n","      <td>0.004682</td>\n","      <td>man fatally shoot los angeles county sheriff d...</td>\n","      <td>man fatally shoot los angeles county sheriff...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       uuid  ...                                         clean_text\n","0  9b265eb1f08a07a093f4e415f4878b5ccec71b0b  ...  washington sophisticated hack pull russia chin...\n","1  0d49c01d17674d04754b87cb7c5167a589934e61  ...  © getty image rachel maddow win grammy audio r...\n","2  ad1d5c62864864e9ae3d36b487cc322f5bcbaf9c  ...    man fatally shoot los angeles county sheriff...\n","\n","[3 rows x 36 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVSc3FrR9_ju","executionInfo":{"status":"ok","timestamp":1619593624308,"user_tz":240,"elapsed":170,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"52192662-6d03-4f28-b425-e1968543842f"},"source":["df_info_all_v2['log_share'].describe()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    20363.000000\n","mean         3.104575\n","std          0.805235\n","min          1.609438\n","25%          2.484907\n","50%          2.995732\n","75%          3.688879\n","max          4.997212\n","Name: log_share, dtype: float64"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"4ciqO-MpXGAi"},"source":["## (1).Check the number of tokens in text"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"17x3PgGp8w6V","executionInfo":{"status":"ok","timestamp":1619580199474,"user_tz":240,"elapsed":217085,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"4c9c95ac-d01a-4704-a702-15f0ac21979a"},"source":["token_lens = []\n","for txt in df_info_all_v2['clean_text']:\n","    tokens = tokenizer.encode(txt, max_length=512)\n","    token_lens.append(len(tokens))\n","sns.distplot(token_lens)\n","plt.xlim([0, 512]);\n","plt.xlabel('Token count');"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"m12ZZqDJ--Ny","executionInfo":{"status":"ok","timestamp":1619580281923,"user_tz":240,"elapsed":318,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["MAX_LEN = 512"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ycLcU550cXPo"},"source":["## (2).Check the number of tokens in title"]},{"cell_type":"code","metadata":{"id":"pTo7C9MwdH4T","executionInfo":{"status":"ok","timestamp":1619580199475,"user_tz":240,"elapsed":200629,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["df_info_all_v2['clean_title'].fillna('', inplace=True)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"T9d5xFBBcW7p","executionInfo":{"status":"ok","timestamp":1619580208824,"user_tz":240,"elapsed":209177,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"7b5468e6-f756-46b7-f72e-9e201e1607cc"},"source":["token_lens = []\n","for txt in df_info_all_v2['clean_title']:\n","    tokens = tokenizer.encode(txt, max_length=512)\n","    token_lens.append(len(tokens))\n","sns.distplot(token_lens)\n","plt.xlim([0, 100]);\n","plt.xlabel('Token count');"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"ggNusbFodqyH","executionInfo":{"status":"ok","timestamp":1619593902277,"user_tz":240,"elapsed":213,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["MAX_TITLE_LEN = 40"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bk6qWm_2ijid"},"source":["# 3.Generate news title\n","It takes lots of time to generate news title, so that this analysis used only 5000 observations."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APrhwX1SiuzD","executionInfo":{"status":"ok","timestamp":1619580934435,"user_tz":240,"elapsed":310,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"375d9479-a8fc-4fbd-ffec-65b2c5f2411c"},"source":["df_info_all_sub = df_info_all_v2.sample(n=5000, random_state=RANDOM_SEED).reset_index(drop=True)\n","df_info_all_sub.shape"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 36)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"sEYJhDQjhzNu"},"source":["gene_text_list = []\n","for i in tqdm(range(df_info_all_sub.shape[0])):\n","    inputs = tokenizer_summary.encode(\"summarize: \" + df_info_all_sub['clean_text'].iloc[i], \n","                                      return_tensors=\"pt\", max_length=MAX_LEN)\n","    outputs = model.generate(inputs, max_length=MAX_TITLE_LEN, min_length=10, \n","                         length_penalty=2.0, num_beams=4, early_stopping=True)\n","    gene_text_list.append(tokenizer_summary.decode(outputs[0])[6:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"M1eOk3yYhJzg","executionInfo":{"status":"ok","timestamp":1619580613394,"user_tz":240,"elapsed":405,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"097161e7-51f6-41ca-80d0-9dbc749d2eda"},"source":["tokenizer_summary.decode(outputs[0])[6:]"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'biden administration congress rethink nation protect grow cyberthreat. washington sophisticated hack pull russia china broad array government industrial target u.s. failure intelligence agency detect drive'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"iEohNoQNTiA9"},"source":["df_info_all_sub['machine_title'] = gene_text_list\n","df_info_all_sub.to_csv('df_all_machine_sub.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPQVsc5PTkeS","executionInfo":{"status":"ok","timestamp":1619593723050,"user_tz":240,"elapsed":374,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"961e9e7a-733c-474d-8228-51ca8b948f0b"},"source":["df_info_all_v2 = pd.read_csv(INPUT_PATH + 'df_all_machine_sub.csv')\n","df_info_all_v2.columns"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['uuid', 'author', 'country', 'published', 'image', 'site',\n","       'site_category', 'page_view', 'fb_comment', 'fb_likes', 'fb_shares',\n","       'linkedin', 'pinterest', 'url', 'date', 'year', 'month', 'day',\n","       'weekday', 'hour', 'minute', 'seccont', 'noweek', 'doc_topic',\n","       'log_share', 'num_title_words', 'num_words', 'num_words_clean',\n","       'country_model', 'site_model', 'country_label', 'site_label',\n","       'log_page', 'positive', 'clean_title', 'clean_text', 'machine_title'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"CG7AsVB8cVj5","executionInfo":{"status":"ok","timestamp":1619593743363,"user_tz":240,"elapsed":357,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"69f3faf3-6e2a-442b-f452-e182b75a0e17"},"source":["df_info_all_v2_oh = pd.get_dummies(df_info_all_v2, columns=['doc_topic', 'country_label', 'site_label'])\n","df_info_all_v2_oh.head(2)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>uuid</th>\n","      <th>author</th>\n","      <th>country</th>\n","      <th>published</th>\n","      <th>image</th>\n","      <th>site</th>\n","      <th>site_category</th>\n","      <th>page_view</th>\n","      <th>fb_comment</th>\n","      <th>fb_likes</th>\n","      <th>fb_shares</th>\n","      <th>linkedin</th>\n","      <th>pinterest</th>\n","      <th>url</th>\n","      <th>date</th>\n","      <th>year</th>\n","      <th>month</th>\n","      <th>day</th>\n","      <th>weekday</th>\n","      <th>hour</th>\n","      <th>minute</th>\n","      <th>seccont</th>\n","      <th>noweek</th>\n","      <th>log_share</th>\n","      <th>num_title_words</th>\n","      <th>num_words</th>\n","      <th>num_words_clean</th>\n","      <th>country_model</th>\n","      <th>site_model</th>\n","      <th>log_page</th>\n","      <th>positive</th>\n","      <th>clean_title</th>\n","      <th>clean_text</th>\n","      <th>machine_title</th>\n","      <th>doc_topic_0</th>\n","      <th>doc_topic_1</th>\n","      <th>doc_topic_2</th>\n","      <th>doc_topic_3</th>\n","      <th>doc_topic_4</th>\n","      <th>doc_topic_5</th>\n","      <th>...</th>\n","      <th>doc_topic_12</th>\n","      <th>doc_topic_13</th>\n","      <th>doc_topic_14</th>\n","      <th>doc_topic_15</th>\n","      <th>doc_topic_16</th>\n","      <th>doc_topic_17</th>\n","      <th>doc_topic_18</th>\n","      <th>doc_topic_19</th>\n","      <th>doc_topic_20</th>\n","      <th>doc_topic_21</th>\n","      <th>doc_topic_22</th>\n","      <th>doc_topic_23</th>\n","      <th>doc_topic_24</th>\n","      <th>country_label_0</th>\n","      <th>country_label_1</th>\n","      <th>country_label_2</th>\n","      <th>country_label_3</th>\n","      <th>country_label_4</th>\n","      <th>country_label_5</th>\n","      <th>country_label_6</th>\n","      <th>country_label_7</th>\n","      <th>country_label_8</th>\n","      <th>country_label_9</th>\n","      <th>country_label_10</th>\n","      <th>site_label_0</th>\n","      <th>site_label_1</th>\n","      <th>site_label_2</th>\n","      <th>site_label_3</th>\n","      <th>site_label_4</th>\n","      <th>site_label_5</th>\n","      <th>site_label_6</th>\n","      <th>site_label_7</th>\n","      <th>site_label_8</th>\n","      <th>site_label_9</th>\n","      <th>site_label_10</th>\n","      <th>site_label_11</th>\n","      <th>site_label_12</th>\n","      <th>site_label_13</th>\n","      <th>site_label_14</th>\n","      <th>site_label_15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>c4b0c443ae3ae0eb514dc343382f317a0ed996e8</td>\n","      <td>Jennifer Barretto, Assistant Editor Features</td>\n","      <td>AE</td>\n","      <td>2021-02-19T11:55:00.000+02:00</td>\n","      <td>1</td>\n","      <td>gulfnews.com</td>\n","      <td>vehicles</td>\n","      <td>12.1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>https://gulfnews.com/entertainment/bollywood/w...</td>\n","      <td>2021-02-19 11:55:00</td>\n","      <td>2021</td>\n","      <td>2</td>\n","      <td>19</td>\n","      <td>Fri</td>\n","      <td>11</td>\n","      <td>55</td>\n","      <td>4276500.0</td>\n","      <td>7</td>\n","      <td>2.079442</td>\n","      <td>15</td>\n","      <td>162</td>\n","      <td>95</td>\n","      <td>AE</td>\n","      <td>gulfnews.com</td>\n","      <td>2.572612</td>\n","      <td>0.981922</td>\n","      <td>watch preity zinta joke shah rukh khan son ary...</td>\n","      <td>actress preity zinta aryan khan son superstar ...</td>\n","      <td>aryan fill famous dad event zinta coowner punj...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0701d5838913fcbe142369d342da61ed1ee3fdad</td>\n","      <td>Emily Tannenbaum</td>\n","      <td>US</td>\n","      <td>2021-02-27T02:00:00.000+02:00</td>\n","      <td>1</td>\n","      <td>www.glamour.com</td>\n","      <td>fashion</td>\n","      <td>2.9</td>\n","      <td>107</td>\n","      <td>766</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>https://www.glamour.com/story/meghan-markle-30...</td>\n","      <td>2021-02-27 02:00:00</td>\n","      <td>2021</td>\n","      <td>2</td>\n","      <td>27</td>\n","      <td>Sat</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4932000.0</td>\n","      <td>8</td>\n","      <td>2.564949</td>\n","      <td>17</td>\n","      <td>238</td>\n","      <td>126</td>\n","      <td>US</td>\n","      <td>other</td>\n","      <td>1.360977</td>\n","      <td>0.432978</td>\n","      <td>meghan markle wear   dress facetime prince har...</td>\n","      <td>meghan markle wear   dress facetime prince har...</td>\n","      <td>meghan markle maternity fashion game lock prin...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 86 columns</p>\n","</div>"],"text/plain":["                                       uuid  ... site_label_15\n","0  c4b0c443ae3ae0eb514dc343382f317a0ed996e8  ...             0\n","1  0701d5838913fcbe142369d342da61ed1ee3fdad  ...             0\n","\n","[2 rows x 86 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Va-HhxveLU7","executionInfo":{"status":"ok","timestamp":1619593746379,"user_tz":240,"elapsed":220,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"fb13df2e-1663-4de0-d26f-0e1bfa670260"},"source":["df_info_all_v2.columns"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['uuid', 'author', 'country', 'published', 'image', 'site',\n","       'site_category', 'page_view', 'fb_comment', 'fb_likes', 'fb_shares',\n","       'linkedin', 'pinterest', 'url', 'date', 'year', 'month', 'day',\n","       'weekday', 'hour', 'minute', 'seccont', 'noweek', 'doc_topic',\n","       'log_share', 'num_title_words', 'num_words', 'num_words_clean',\n","       'country_model', 'site_model', 'country_label', 'site_label',\n","       'log_page', 'positive', 'clean_title', 'clean_text', 'machine_title'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b8-4KqmfPtt","executionInfo":{"status":"ok","timestamp":1619593747190,"user_tz":240,"elapsed":180,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"0324333f-23e6-45ea-efe3-7f3bd4b59724"},"source":["df_info_all_v2_oh.columns"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['uuid', 'author', 'country', 'published', 'image', 'site',\n","       'site_category', 'page_view', 'fb_comment', 'fb_likes', 'fb_shares',\n","       'linkedin', 'pinterest', 'url', 'date', 'year', 'month', 'day',\n","       'weekday', 'hour', 'minute', 'seccont', 'noweek', 'log_share',\n","       'num_title_words', 'num_words', 'num_words_clean', 'country_model',\n","       'site_model', 'log_page', 'positive', 'clean_title', 'clean_text',\n","       'machine_title', 'doc_topic_0', 'doc_topic_1', 'doc_topic_2',\n","       'doc_topic_3', 'doc_topic_4', 'doc_topic_5', 'doc_topic_6',\n","       'doc_topic_7', 'doc_topic_8', 'doc_topic_9', 'doc_topic_10',\n","       'doc_topic_11', 'doc_topic_12', 'doc_topic_13', 'doc_topic_14',\n","       'doc_topic_15', 'doc_topic_16', 'doc_topic_17', 'doc_topic_18',\n","       'doc_topic_19', 'doc_topic_20', 'doc_topic_21', 'doc_topic_22',\n","       'doc_topic_23', 'doc_topic_24', 'country_label_0', 'country_label_1',\n","       'country_label_2', 'country_label_3', 'country_label_4',\n","       'country_label_5', 'country_label_6', 'country_label_7',\n","       'country_label_8', 'country_label_9', 'country_label_10',\n","       'site_label_0', 'site_label_1', 'site_label_2', 'site_label_3',\n","       'site_label_4', 'site_label_5', 'site_label_6', 'site_label_7',\n","       'site_label_8', 'site_label_9', 'site_label_10', 'site_label_11',\n","       'site_label_12', 'site_label_13', 'site_label_14', 'site_label_15'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"gClCrC5Reg1z","executionInfo":{"status":"ok","timestamp":1619593810710,"user_tz":240,"elapsed":183,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["col = ['image','doc_topic',\n","       'log_share', 'num_words_clean',\n","       'country_label', 'site_label',\n","       'log_page', 'positive', 'machine_title']"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"juBAPNT8fOTp","executionInfo":{"status":"ok","timestamp":1619593808838,"user_tz":240,"elapsed":179,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["drop_x = ['uuid', 'author', 'country', 'published', 'site',\n","          'site_category', 'page_view', 'fb_comment', 'fb_likes', \n","          'fb_shares', 'linkedin', 'pinterest', 'url', 'date', \n","          'year', 'month', 'day','weekday', 'hour', 'minute', \n","          'seccont', 'noweek', 'num_title_words', 'num_words', \n","          'country_model', 'site_model', 'clean_title', 'clean_text']"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MFXrYxc8TgmY","executionInfo":{"status":"ok","timestamp":1619593812624,"user_tz":240,"elapsed":770,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"def1995d-0ce1-4b37-86ed-1e1f07eb24eb"},"source":["df_model = df_info_all_v2[col]\n","df_model_oh = df_info_all_v2_oh.drop(drop_x, axis=1)\n","df_model_oh.columns"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['image', 'log_share', 'num_words_clean', 'log_page', 'positive',\n","       'machine_title', 'doc_topic_0', 'doc_topic_1', 'doc_topic_2',\n","       'doc_topic_3', 'doc_topic_4', 'doc_topic_5', 'doc_topic_6',\n","       'doc_topic_7', 'doc_topic_8', 'doc_topic_9', 'doc_topic_10',\n","       'doc_topic_11', 'doc_topic_12', 'doc_topic_13', 'doc_topic_14',\n","       'doc_topic_15', 'doc_topic_16', 'doc_topic_17', 'doc_topic_18',\n","       'doc_topic_19', 'doc_topic_20', 'doc_topic_21', 'doc_topic_22',\n","       'doc_topic_23', 'doc_topic_24', 'country_label_0', 'country_label_1',\n","       'country_label_2', 'country_label_3', 'country_label_4',\n","       'country_label_5', 'country_label_6', 'country_label_7',\n","       'country_label_8', 'country_label_9', 'country_label_10',\n","       'site_label_0', 'site_label_1', 'site_label_2', 'site_label_3',\n","       'site_label_4', 'site_label_5', 'site_label_6', 'site_label_7',\n","       'site_label_8', 'site_label_9', 'site_label_10', 'site_label_11',\n","       'site_label_12', 'site_label_13', 'site_label_14', 'site_label_15'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"zZGk6-oBhhnn"},"source":["# 4.Create Network with machine generated title"]},{"cell_type":"markdown","metadata":{"id":"qFESFzCzlGxJ"},"source":["## (1).Dataset"]},{"cell_type":"code","metadata":{"id":"p3HzzTj7--Le","executionInfo":{"status":"ok","timestamp":1619593852012,"user_tz":240,"elapsed":208,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["class NewsDataset(Dataset):\n","    def __init__(self, title, df, shares, tokenizer, max_len):\n","        self.title = title\n","        self.shares = shares\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.df = df\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, item):\n","        title = str(self.title[item])\n","        shares = self.shares[item]\n","        meta = self.df.iloc[item].values\n","        encoding = self.tokenizer.encode_plus(\n","            title,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","            )\n","        return {\n","            'title': title,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'meta': torch.tensor(meta,dtype=torch.float),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'shares': torch.tensor(shares, dtype=torch.float)\n","            }"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VCzeaCF--JV","executionInfo":{"status":"ok","timestamp":1619593889886,"user_tz":240,"elapsed":346,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["df_train, df_test = train_test_split(\n","    df_model_oh,\n","    test_size=0.2,\n","    random_state=RANDOM_SEED\n","    )\n","df_val, df_test = train_test_split(\n","    df_test,\n","    test_size=0.5,\n","    random_state=RANDOM_SEED\n","    )"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"zDbKixFwFKro","executionInfo":{"status":"ok","timestamp":1619593892160,"user_tz":240,"elapsed":189,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["def create_data_loader(df_all, tokenizer, max_len, batch_size):\n","    ds = NewsDataset(\n","            title = df_all['machine_title'].to_numpy(),\n","            shares = df_all['log_share'].to_numpy(),\n","            tokenizer = tokenizer,\n","            df = df_all.drop(['log_share', 'machine_title'], axis=1),\n","            max_len = max_len\n","    )\n","    return DataLoader(\n","            ds,\n","            batch_size=batch_size,\n","            num_workers=2\n","        )"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ds-5khMa--Ea","executionInfo":{"status":"ok","timestamp":1619593911474,"user_tz":240,"elapsed":187,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["BATCH_SIZE = 16\n","train_data_loader = create_data_loader(df_train, tokenizer, MAX_TITLE_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(df_val, tokenizer, MAX_TITLE_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_TITLE_LEN, BATCH_SIZE)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P4fegm_blJcK"},"source":["## (2).Network"]},{"cell_type":"code","metadata":{"id":"bKA_DzoW--Bk","executionInfo":{"status":"ok","timestamp":1619593925488,"user_tz":240,"elapsed":614,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["class NewsRegressor(nn.Module):\n","    def __init__(self):\n","        super(NewsRegressor, self).__init__()\n","        self.bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n","                                                                        num_labels = 1)\n","        self.mlp = nn.Sequential(\n","                          nn.Linear(57, 100),\n","                          nn.BatchNorm1d(100),\n","                          nn.Dropout(0.3),\n","                          nn.ReLU(),\n","                          nn.Linear(100, 1),\n","                          )\n","    def forward(self, input_ids, attention_mask, meta):\n","        bert_output = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask\n","            )\n","        input = torch.cat((meta, bert_output.logits), axis=1)\n","        out_mlp = self.mlp(input)\n","        return out_mlp"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["8a0e754b395046048b8a08fa017e25ca","72d7ae2aff1e4953b0ed50d6f2bcabb5","5517a94f4e1a46a587ed036b1e4beb72","a64fabe17dd34f898ae7872b11b2b36c","bb66c1859fad4f5e9a2b6c7df33b62c0","ee47569e14f5404b99cde357182e34d1","c85765bfef0a42bfb3736e7f03e195bc","f308cecdfe95410981c723ac8de5d3d3","0a7fb51f904545b485ec07efb00aa373","3f137185bb9646d7af24ea830b32aff6","c3145a6956db46b4972030d662b6eadf","e43396fcd6ce4aa68ddccb71570fa6f8","9100dac0feac4569a64463501351567b","80bcda2cae024cc0bfe9ab28ac608eda","7b9af5e924d046f481120de07f2103a2","f409d49bc78c4e14aae693f36321efff"]},"id":"XeMSdTBI-9_G","executionInfo":{"status":"ok","timestamp":1619593946362,"user_tz":240,"elapsed":13143,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"8a23ba89-05a1-42e0-b439-8fd249391f49"},"source":["model_news = NewsRegressor()\n","model_news = model_news.to(device)"],"execution_count":33,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a0e754b395046048b8a08fa017e25ca","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a7fb51f904545b485ec07efb00aa373","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iPgRV_KwNDGG","executionInfo":{"status":"ok","timestamp":1619573349627,"user_tz":240,"elapsed":367,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"7c9b57d5-95f8-4945-e75a-8c8736c39b8e"},"source":["gc.collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["538"]},"metadata":{"tags":[]},"execution_count":203}]},{"cell_type":"code","metadata":{"id":"SbogHnP--98f","executionInfo":{"status":"ok","timestamp":1619593946362,"user_tz":240,"elapsed":3717,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["EPOCHS = 30\n","optimizer = AdamW(model_news.parameters(), lr=1e-5)\n","total_steps = len(train_data_loader) * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","loss_fn = nn.MSELoss().to(device)"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fjc4JOWZ-951","executionInfo":{"status":"ok","timestamp":1619593954214,"user_tz":240,"elapsed":185,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["def train_epoch(model, data_loader,\n","                loss_fn, optimizer,\n","                device, scheduler):\n","    model = model.train()\n","    losses = []\n","    for d in data_loader:\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        shares = d[\"shares\"].to(device)\n","        meta = d['meta'].to(device)\n","        outputs = model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            meta = meta\n","            )\n","        loss = loss_fn(outputs, shares)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","    return np.mean(losses)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"GPiIAEn5-928","executionInfo":{"status":"ok","timestamp":1619593957061,"user_tz":240,"elapsed":284,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}}},"source":["def eval_model(model, data_loader, loss_fn, device):\n","    model = model.eval()\n","    losses = []\n","    with torch.no_grad():\n","        for d in data_loader:\n","            input_ids = d[\"input_ids\"].to(device)\n","            attention_mask = d[\"attention_mask\"].to(device)\n","            shares = d[\"shares\"].to(device)\n","            meta = d['meta'].to(device)\n","            outputs = model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                meta=meta\n","            )\n","        loss = loss_fn(outputs, shares)\n","        losses.append(loss.item())\n","    return np.mean(losses)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZcQd0to-9zo","executionInfo":{"status":"ok","timestamp":1619594411764,"user_tz":240,"elapsed":444987,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"d29332d8-0e84-409e-ede3-b2dea604f74c"},"source":["%%time\n","history_bert = defaultdict(list)\n","best_loss = np.inf\n","for epoch in range(EPOCHS):\n","    print(f'Epoch {epoch + 1}/{EPOCHS}')\n","    print('-' * 10)\n","    train_loss = train_epoch(\n","        model_news,\n","        train_data_loader,\n","        loss_fn,\n","        optimizer,\n","        device,\n","        scheduler\n","    )\n","    print(f'Train loss {train_loss}')\n","    val_loss = eval_model(\n","        model_news,\n","        val_data_loader,\n","        loss_fn,\n","        device\n","    )\n","    print(f'Val loss {val_loss}')\n","    print()\n","    history_bert['train_loss'].append(train_loss)\n","    history_bert['val_loss'].append(val_loss)\n","    if val_loss < best_loss:\n","        torch.save(model_news.state_dict(), 'best_model_bert_state.bin')\n","        best_loss = val_loss"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 13.20163475036621\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 10.222555160522461\n","\n","Epoch 2/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 12.463893192291259\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 9.516664505004883\n","\n","Epoch 3/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 11.648999526977539\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 8.877196311950684\n","\n","Epoch 4/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 10.98939125442505\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 8.277599334716797\n","\n","Epoch 5/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 10.266011911392212\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 7.771642684936523\n","\n","Epoch 6/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 9.744571765899659\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 7.339076042175293\n","\n","Epoch 7/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 9.186304187774658\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 6.958795070648193\n","\n","Epoch 8/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 8.715750118255615\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 6.624858856201172\n","\n","Epoch 9/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 8.314183570861816\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 6.341978073120117\n","\n","Epoch 10/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 7.944635360717774\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 6.105213165283203\n","\n","Epoch 11/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 7.577085256576538\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 5.857022285461426\n","\n","Epoch 12/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 7.432509824752808\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 5.566037178039551\n","\n","Epoch 13/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 7.141919841766358\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 5.496021270751953\n","\n","Epoch 14/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 6.951239700317383\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 5.377811908721924\n","\n","Epoch 15/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 6.7284399585723875\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 5.19975471496582\n","\n","Epoch 16/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 6.539142845153808\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 5.131958961486816\n","\n","Epoch 17/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 6.354122075080872\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.942637920379639\n","\n","Epoch 18/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 6.15992281627655\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.860247611999512\n","\n","Epoch 19/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 6.0464134006500245\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.747271537780762\n","\n","Epoch 20/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.889661664962769\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.654791355133057\n","\n","Epoch 21/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.742522908210755\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.599359512329102\n","\n","Epoch 22/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.611804245948791\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.53996467590332\n","\n","Epoch 23/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.540951930999756\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.469381809234619\n","\n","Epoch 24/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.454450473785401\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.442503929138184\n","\n","Epoch 25/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.363129193305969\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.387855529785156\n","\n","Epoch 26/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.284782383918762\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.357733726501465\n","\n","Epoch 27/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.258710717201233\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.311827659606934\n","\n","Epoch 28/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.189207761764527\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.291314125061035\n","\n","Epoch 29/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.1964317951202394\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.29542350769043\n","\n","Epoch 30/30\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Train loss 5.164441646575928\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Val loss 4.288273811340332\n","\n","CPU times: user 5min 25s, sys: 1min 35s, total: 7min 1s\n","Wall time: 7min 24s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"1DXRY_zTnMIH","executionInfo":{"status":"ok","timestamp":1619594420978,"user_tz":240,"elapsed":587,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"65e4f7fd-c62d-4b9e-d1fa-5539a8c6df9b"},"source":["plt.plot(history_bert['train_loss'], label='train loss')\n","plt.plot(history_bert['val_loss'], label='validation loss')\n","plt.title('Training history')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend()"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f591e9d8b50>"]},"metadata":{"tags":[]},"execution_count":38},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uMIBUIVPtCvt","executionInfo":{"status":"ok","timestamp":1619594429109,"user_tz":240,"elapsed":853,"user":{"displayName":"HM","photoUrl":"","userId":"12637397841890976833"}},"outputId":"6e123b23-3cc1-4ec6-9e90-ae148138a8ef"},"source":["with torch.no_grad():\n","    prediction = []\n","    shares_list = []\n","    for i, d in enumerate(test_data_loader):\n","        input_ids = d[\"input_ids\"].to(device)\n","        attention_mask = d[\"attention_mask\"].to(device)\n","        shares = d[\"shares\"].to(device)\n","        meta = d['meta'].to(device)\n","        outputs = model_news(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            meta=meta\n","        )\n","        shares_list += shares.tolist()\n","        prediction += outputs.flatten().tolist()\n","mean_squared_error(prediction, shares_list)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["6.573651876901202"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"i0geHoOFGVLN"},"source":[],"execution_count":null,"outputs":[]}]}